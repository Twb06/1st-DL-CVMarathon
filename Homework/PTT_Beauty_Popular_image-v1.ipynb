{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#送出request，並接收response\n",
    "def RequestResponse(url):\n",
    "    cookies = {\"over18\" : \"1\"}\n",
    "    r = requests.get(url, cookies = cookies)\n",
    "    r.encoding = \"utf-8\"\n",
    "    respond = r.text\n",
    "    \n",
    "    return respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"hl f3\">30</span>, <span class=\"hl f3\">14</span>, <span class=\"hl f3\">40</span>, <span class=\"hl f3\">18</span>, <span class=\"hl f3\">11</span>, <span class=\"hl f3\">25</span>, <span class=\"hl f3\">75</span>]\n",
      "['/bbs/Beauty/M.1581696638.A.F13.html', '/bbs/Beauty/M.1581725717.A.9B5.html', '/bbs/Beauty/M.1581738006.A.63B.html', '/bbs/Beauty/M.1581743347.A.082.html', '/bbs/Beauty/M.1581747004.A.897.html', '/bbs/Beauty/M.1581760367.A.920.html']\n"
     ]
    }
   ],
   "source": [
    "#針對Beauty版\n",
    "url = \"https://www.ptt.cc/bbs/Beauty/index.html\"\n",
    "while True:    \n",
    "    #以BeautifulSoup解析html\n",
    "    soup_beauty = BeautifulSoup(RequestResponse(url), \"html5lib\")\n",
    "    #print(soup.prettify())\n",
    "\n",
    "    #尋找所有帖子存在的標籤\n",
    "    contents = soup_beauty.find(\"div\", class_ = \"r-list-container action-bar-margin bbs-screen\")\n",
    "    #print(contents.prettify())\n",
    "\n",
    "    #找到推文數>=11的帖子，class中hl f3代表推文數11~99，hl f1代表推爆\n",
    "    post_popularity_list = contents.find_all(\"span\", class_ = \"hl f3\" or \"hl f1\")\n",
    "    print(post_popularity_list)\n",
    "\n",
    "    #ptt第一頁下方會有公告，位置會在<div class=\"r-list-sep\"></div>分隔線的下方\n",
    "    #排除掉推文數高的公告\n",
    "    bulletin_check = post_popularity_list[-1].find_all_previous(\"div\", class_ = \"r-list-sep\")\n",
    "    while bulletin_check != []:\n",
    "        if len(list(post_popularity_list)) == 1:\n",
    "            url = \"https://www.ptt.cc{}\".format(soup_beauty.find(string = \"‹ 上頁\").find_parent(\"a\")[\"href\"])\n",
    "            print(url)\n",
    "            break\n",
    "        del post_popularity_list[-1]\n",
    "        bulletin_check = post_popularity_list[-1].find_all_previous(\"div\", class_ = \"r-list-sep\")\n",
    "    \n",
    "    if len(list(post_popularity_list)) == 1:\n",
    "        print(\"This page ain't no good shit:( Trying previous page...\")\n",
    "        continue\n",
    "\n",
    "    #找出推文數各自對應的帖子連結\n",
    "    post_url_list = []\n",
    "    for post_popularity in post_popularity_list:\n",
    "        post_url_list.append(post_popularity.find_next(\"a\")[\"href\"])\n",
    "\n",
    "    print(post_url_list)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./photos\\S7yX1Uo.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\4lAdfUW.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\oalbrQy.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\67tKYda.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\BTyiJJQ.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\r8j7e2V.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\WLF8fbk.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\102D2LD.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\ouOIDD4.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\R8EjtS9.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\3euLhng.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\hoqnx5z.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\j1Vx5lZ.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\d5el9M8.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\TQiSv3o.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\MQT42r4.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\lpoo3SA.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\8xOTks5.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\FUnppU2.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\D76CLvh.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\QPS6zeR.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\VCGTZE3.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\dpO1cSZ.gif has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\xLRz2VL.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\y3NcRPr.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\PORwGFi.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\I0Kba6X.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\R2WSDsT.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\ZckmIT4.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\nJNV8xj.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\g407NVY.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\YFn1hLr.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\9quA6q0.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\n4Y8TBD.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\fKe8KUa.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\vsMrJBM.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\cPVFONG.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\mZRcwFZ.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\kpnYYqz.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\ON8rlLu.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\GKo21Cc.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\6fXMj8S.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\ExaVL7q.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\p0fgKPB.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\kUTPWZw.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\ehHsrTD.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\UGiHKiP.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\CcnInGx.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\8Thm6QN.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\gHrGE42.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\DhyWXtz.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "./photos\\rqxQM3i.jpeg has been already existed!\n",
      "Continue to download other images...\n",
      "total time consumed: 121.56901979446411\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "#下載圖片，傳入檔名與副檔名(eg. xxxx.jpg)\n",
    "def DownloadBeautyImage(img_id):\n",
    "    \n",
    "    # 決定要儲存的資料夾\n",
    "    output_dir = './photos'\n",
    "    \n",
    "    # 假如資料夾不存在就新增一個資料夾\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # 要request的圖片的url\n",
    "    img_url = \"https://i.imgur.com/{}\".format(img_id)\n",
    "    \n",
    "    \n",
    "    #buffer = tempfile.SpooledTemporaryFile(max_size=1e9)\n",
    "    \n",
    "    # 對圖片送出請求，設定為 stream 之後會跟網站建立通道\n",
    "    with requests.get(img_url, stream = True) as r:\n",
    "        \n",
    "        # r.raise_for_status()會先檢查request是否成功，成功則不會有影響，失敗則會發起httperror\n",
    "        try:\n",
    "            r.raise_for_status()\n",
    "\n",
    "            #local_file_path = \"./photos/{}\".format(img_id)\n",
    "            \n",
    "            # 透過 loop 把檔案拆成很多 chunk 下載\n",
    "            # 先下載到緩衝區內，確定附檔名後再儲存\n",
    "            with tempfile.SpooledTemporaryFile(max_size=1e9) as buffer:\n",
    "                for chunk in r.iter_content(chunk_size = 8192):\n",
    "                    if chunk:\n",
    "                        buffer.write(chunk)\n",
    "                \n",
    "                buffer.seek(0)\n",
    "                \n",
    "                img = Image.open(io.BytesIO(buffer.read()))\n",
    "                local_file_path = os.path.join(output_dir, \"{img_name}.{img_ext}\".format(\n",
    "                    img_name = img_id.split(\".\")[0], img_ext = img.format.lower()))\n",
    "                img.save(local_file_path)\n",
    "                \"\"\"if not os.path.isfile(local_file_path):\n",
    "                    img.save(local_file_path)\n",
    "                else:\n",
    "                    print(\"{} has been already existed!\".format(local_file_path))\n",
    "                    return 0\"\"\"\n",
    "            # 透過 loop 把檔案拆成很多 chunk 下載\n",
    "            \"\"\"with open(local_file_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size = 8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \"\"\"\n",
    "            # 此處才進行檔案格式分析，因為若在前面開啟，會把網頁的respond搶走，使loop不會接收到任何資料\n",
    "            # 如果此處Image.open()打開的是r.raw也會因為loop把網頁的respond搶走，使Image.open()不會接收到任何資料\n",
    "            # ...\n",
    "            \"\"\"with Image.open(local_file_path) as img:\n",
    "                img_ext = img.format.lower()\n",
    "\n",
    "            new_local_file_path =  \"{img_root}.{img_ext}\".format(\n",
    "                    img_root = os.path.splitext(local_file_path)[0], img_ext = img_ext)\n",
    "            if local_file_path != new_local_file_path:\n",
    "                try:\n",
    "                    os.rename(local_file_path, new_local_file_path)\n",
    "                except OSError as error:\n",
    "                    if type(error) == FileExistsError:\n",
    "                        os.remove(local_file_path)\n",
    "                    print(error)\n",
    "                    \n",
    "                    return 0\n",
    "                else:\n",
    "                    print(\"Sucess\")\"\"\"\n",
    "        \n",
    "        # request失敗\n",
    "        except requests.exceptions.HTTPError as internet_error:\n",
    "            print(internet_error)\n",
    "            \n",
    "            return 0\n",
    "    \n",
    "    return local_file_path\n",
    "    \n",
    "\n",
    "Start_time = time.time()\n",
    "    \n",
    "for post_url in post_url_list:\n",
    "    response = RequestResponse(\"http://www.ptt.cc{}\".format(post_url))\n",
    "    soup_beauty_post = BeautifulSoup(response, \"html5lib\")\n",
    "    \n",
    "    img_tag_list = soup_beauty_post.find(id = \"main-content\").findChildren(\"a\", recursive = False)\n",
    "    \n",
    "    for img_tag in img_tag_list:\n",
    "        \n",
    "        # 取得所有圖片在第三方服務的 id\n",
    "        if 'imgur' not in img_tag['href']:\n",
    "            continue\n",
    "        \n",
    "        # 取出檔名與副檔名(eg. xxxx.jpg)\n",
    "        img_id = img_tag[\"href\"].split(\"/\")[-1]\n",
    "\n",
    "        # 呼叫自定義下載圖片的函式，傳入檔名與副檔名(eg. xxxx.jpg)，傳回所下載的檔案的路徑\n",
    "        local_file_path = DownloadBeautyImage(img_id)\n",
    "        \n",
    "        # local_file_path == 0代表Error，印出失敗訊息並繼續下載\n",
    "        if local_file_path == 0:\n",
    "            print(\"Continue to download other images...\")\n",
    "            continue\n",
    "        # 其他情況代表成功，印出成功訊息並繼續下載\n",
    "        print(local_file_path)\n",
    "        local_file_name = local_file_path.split(\"/\")[-1]\n",
    "        print(\"{local_file_name} has been saved in {local_file_path}\".format(\n",
    "            local_file_name = local_file_name, local_file_path = os.path.abspath(local_file_path)))\n",
    "        \n",
    "End_time = time.time()\n",
    "\n",
    "print(\"total time consumed: {}\".format(End_time - Start_time))\n",
    "#新的下載124s, 112s\n",
    "#檢查是否覆蓋檔案119s, 121s\n",
    "#不檢查115s, 108s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "404 Client Error: Not Found for url: https://imgur.com/9yk9sk8/88\n"
     ]
    }
   ],
   "source": [
    "\"\"\"url = \"https://i.imgur.com/9yk9sk8/88.jpg\"\n",
    "\n",
    "with requests.get(url, stream = True) as r:\n",
    "    try:    \n",
    "        r.raise_for_status()\n",
    "            \n",
    "        local_file_path = \"./photos/{}\".format(url.split(\"/\")[-1])\n",
    "        \n",
    "        with open(local_file_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size = 8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    \n",
    "        with Image.open(local_file_path) as img:\n",
    "            img_ext = img.format.lower()\n",
    "        \n",
    "        new_local_file_path =  \"{img_root}.{img_ext}\".format(\n",
    "                img_root = os.path.splitext(local_file_path)[0], img_ext = img_ext)\n",
    "        if local_file_path != new_local_file_path:\n",
    "            try:\n",
    "                os.rename(local_file_path, new_local_file_path)\n",
    "            except OSError as e:\n",
    "                if type(e) == FileExistsError:\n",
    "                    os.remove(local_file_path)\n",
    "                print(e)\n",
    "            else:\n",
    "                print(\"Sucess\")\n",
    "                print(os.path.abspath(new_local_file_path))\n",
    "    \n",
    "    except requests.exceptions.HTTPError as internet_error:\n",
    "        print(internet_error)\n",
    "        print(\"Continue to download other image...\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
